% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  authoryear,
  preprint,
  3p]{elsarticle}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{fvextra}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
\DefineVerbatimEnvironment{OutputCode}{Verbatim}{breaklines,commandchars=\\\{\}}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\journal{CPSA}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{elsarticle-harv}
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Beyond Multiple Choices},
  pdfauthor={Laurence-Olivier M. Foisy; Hubert Cadieux; Yannick Dufresne},
  pdfkeywords={Large Language Models, Open-Ended Survey
Questions, Survey Research, Open-Source Tools},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\setlength{\parindent}{6pt}
\begin{document}

\begin{frontmatter}
\title{Beyond Multiple Choices \\\large{Capturing Nuanced Public Opinion
with Large Language Models} }
\author[1]{Laurence-Olivier M. Foisy%
\corref{cor1}%
}
 \ead{mail@mfoisy} 
\author[1]{Hubert Cadieux%
%
}
 \ead{hubert.cadieux.1@ulaval.ca} 
\author[1]{Yannick Dufresne%
%
}
 \ead{yannick.dufresne@pol.ulaval.ca} 

\affiliation[1]{organization={Université Laval, Département de science
politique},addressline={2325 Rue de l'Université, Québec, QC G1V
0A6},city={Québec},postcode={G1V 0A6},postcodesep={}}

\cortext[cor1]{Corresponding author}



        
\begin{abstract}
Analyzing open-ended survey questions presents significant challenges
due to the diversity of responses and the manual effort required for
coding and categorization. This paper introduces a novel approach for
cleaning and analyzing open-ended questions using open-source large
language models (LLMs). Leveraging the R programming language and
Ollama's API, we demonstrate an efficient, cost-effective method for
processing qualitative data. Our approach enhances the ability to
extract meaningful insights from survey responses, providing a scalable
solution for researchers. By integrating open-source tools, we offer a
practical framework for transforming the analysis of open-ended
questions in survey research.
\end{abstract}





\begin{keyword}
    Large Language Models \sep Open-Ended Survey Questions \sep Survey
Research \sep 
    Open-Source Tools
\end{keyword}
\end{frontmatter}
    
\section{Introduction}\label{introduction}

Open-ended survey questions are notoriously difficult to analyze. In
1932, Rensis Likert published a seminal article in the Archives of
Psychology, introducing a new method to measure the intensity of
agreement or disagreement with a statement. The author underlined the
difficulty of measuring attitudes. He wrote that ``since it is possible
to group stimuli in almost any conceivable manner and to classify and
subclassify them indefinitely, it is strictly true that the number of
attitudes which any given person possesses is almost infinite''
\citep{likert32}. Indeed, implementing open-ended questions in a survey
comes with a host of challenges. Respondents often skip them because
they are time-consuming and require more effort and reflection than
closed-ended questions. It can also be troublesome for mobile users to
type lengthy and complex responses \citep{dillman_etal14}. Open-ended
questions are also difficult to analyze because they require manual
coding and categorization of the responses. Indeed, many respondents can
give the same answer written in different ways. In a 2024 pilot survey
about lifestyle and health given to 2000 French and English Canadian
respondents, people were asked, ``What is your favourite band or
musician?'' The most popular answer chosen by 75 respondents, The
Beatles, was written in 10 different ways: the beatles, The Beatles, The
beatles, The Bwatles, beatles, Beatles, beetles, Beetles, les beattels,
and Les Beatles. Grammatical errors, typos, and misspellings can make it
difficult to analyze the data. While it is not impossible to analyze
open-ended questions, it is generally time-consuming and expensive,
especially when dealing with large datasets
\citep{dillman_etal14, bradburn_etal04}.

However, open-ended questions can provide valuable insights into the
attitudes, opinions, and perceptions of respondents. They allow for more
detailed and nuanced responses than closed-ended questions. They avoid
the problem of forcing respondents to choose between a limited number of
options, which may not capture the full range of their opinions
\citep{dillman_etal14}. Open-ended questions can help researchers to
better understand the attitudes and opinions of respondents and to
identify emerging issues and trends.

According to \citet{bickman_rog09}, open-ended questions align with
qualitative data, requiring in-depth analysis and interpretation, while
closed-ended questions are more suited to quantitative analysis due to
their ease of bulk analysis. This paper proposes utilizing current AI
technology to facilitate the analysis and quantification of open-ended
questions using the R programming language and Ollama's API. By doing
so, we can bridge the gap between qualitative data and quantitative
methods, enabling more comprehensive and insightful analysis from survey
respondents, ultimately enhancing our understanding of complex social
phenomena.

\section{Research Question}\label{research-question}

Can open-source language models be trusted to accurately clean and
analyze open-ended survey questions? The goal is to provide a practical
solution for streamlining the analysis of open-ended questions in survey
research. By using open-source tools and integrating them into the
conventional social scientist's toolkit, we aim to enhance the ability
to extract meaningful insights from survey responses, providing an
easy-to-implement solution for researchers. The goal is not to replace
other methods of analyzing open-ended questions but to provide and
validate an alternative that could be used in conjunction with other
methods to improve the quality of the analysis.

\section{Methodology}\label{methodology}

\subsection{Ollama}\label{ollama}

Ollama is an open source platform that provides a user-friendly way of
downloading and running LLMs locally. It runs a server on the user's
machine that can be accessed through an API. Doing so allows the user to
interact with various LLMs without the need for extensive technical
expertise or reliance on cloud-based platforms. The Ollama API combined
with their library of pre-trained models is a powerful tool that can be
used to generate text, summarize documents, and perform a wide range of
other natural language processing tasks for free. The API is designed to
be easy to use and flexible, allowing users to customize their
interactions with LLMs to suit their needs. Using this tool, we can
potentially clean and analyze open-ended survey questions, even with
limited resources.

Ollama offers a wide range of models in their library. Smaller models
which can be used with a typical laptop and larger models requiring more
computational power and GPUs. Ollama recommends a minimum of 8GB of RAM
to run smaller 7 Billion parameters models and 16GB of RAM to run larger
13 Billion parameters models \citep{ollama24}.

\subsection{The CLELLM Package}\label{the-clellm-package}

For the purpose of this paper, we built a R package that allows
researchers to interact with various language models through Ollama's
API. A similar package has already been published on CRAN by
\citet{gruber_weber24}. Users can install it directly using
\texttt{install.packages("rollama")} However, the package presented in
this paper is a little bit more flexible, and intuitive to use for
non-ai-specialists, allowing to easily install new model and to
alternate them between each prompts making it easier to pick the best
model for each respective task.

To install the package, users can use the following function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{devtools}\SpecialCharTok{::}\FunctionTok{install\_github}\NormalTok{(}\StringTok{"clessn/clellm"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Linux users can use the following function to install ollama:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{clellm}\SpecialCharTok{::}\FunctionTok{install\_ollama}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Windows and MacOS users can download the ollama binary from the
\href{https://ollama.com/}{Ollama website} and install it manually. Once
Ollama is installed, the user can pull the models they want to use
directly in R with the following function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{clellm}\SpecialCharTok{::}\FunctionTok{ollama\_install\_model}\NormalTok{(}\StringTok{"model\_name"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The package provides a simple functions that allow users to interact
with LLMs through Ollama's API. It is designed to be easy to use and
flexible, allowing users to interact with a wide collection of
open-source models the same way they would typically interact with
OpenAI's GPT models.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{clellm}\SpecialCharTok{::}\FunctionTok{ollama\_prompt}\NormalTok{(}\StringTok{"prompt"}\NormalTok{, }\AttributeTok{model =} \StringTok{"model\_name"}\NormalTok{, }\AttributeTok{format =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{print\_result =} \ConstantTok{TRUE}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

The functions takes in four arguments, the prompt, the model to use, the
format of the output, and whether to print the result. The prompt is the
text that the model will use to generate a response. The model is the
name of the model to use. The format is the format of the output, which
can be either ``json'' or ``text''. The print\_result argument is a
logical value that determines whether the result should be printed to
the console. The function returns the response generated by the model.

\subsection{Cleaning Open-Ended
Questions}\label{cleaning-open-ended-questions}

Cleaning open-ended questions is a crucial step in the analysis of
survey data. It involves removing irrelevant information, correcting
errors, and standardizing the responses. This process can be
time-consuming and labor-intensive, especially when dealing with large
datasets. However, by using LLMs, we can automate much of this process,
making it faster and more efficient.

To validate if open-source LLMs with limited parameters can be used to
clean open-ended questions, we will run issue categorization on a subset
of 200 respondents drawn randomly from the 2021 Canadian Election Study.
The respondents were asked ``What is the most important issue to you
personally in this federal election?'' and could answer freely. The goal
of the analysis is to classify the responses into a set of
pre-determined issue categories. For this paper, the 12 issues from
ULaval's Center for Public Policy Analysis (CAPP) were used but any set
of categories could be used. A human coder classified the open-ended
responses into the 12 categories which will serve as a benchmark for the
model's performance. For the purpose of this paper, the score of the
human coder will be considered as the ground truth and set at 100\%. The
model's performance will be measured by comparing its classification to
the human coder's classification.

To validate if open-source large language models (LLMs) with limited
parameters can be used to clean open-ended questions, we will run issue
categorization on a subset of 200 respondents drawn randomly from the
2021 Canadian Election Study. The respondents were asked, ``What is the
most important issue to you personally in this federal election?'' and
could answer freely. The goal of the analysis is to classify the
responses into a set of predetermined issue categories. For this paper,
the 12 issues from ULaval's Center for Public Policy Analysis (CAPP)
were used, although any set of categories could be employed. Depending
on the needs of the research or the researcher, any set of categories
can be utilized, providing flexibility and adaptability in the analysis.

A human coder classified the open-ended responses into the 12
categories, which will serve as a benchmark for the model's performance.
For the purpose of this paper, the score of the human coder will be
considered the ground truth and set at 100\%. The model's performance
will be measured by comparing its classification to the human coder's
classification.

Three open-source models will be asked to classify the responses:
mistral from \citet{jiang_etal23}, llama3 from \citet{meta24}, and
phi3:mini from \citet{abdin_etal24}. For comparison, gpt-4-turbo from
\citet{openai23b}, the most capable model available on the market at the
moment of writing this paper, will also be given the same task. Although
the quality of its ouputs are notoriously superior to any other models,
it is not free to use and requires paid API access to run.

Each models will be asked to classify the responses into one of the 12
issue categories. They will all be given the exact same prompt.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prompt }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\StringTok{"In this survey question, respondents had to name their most important issue. Please read the answer and determine to which of the following "}\NormalTok{,}\FunctionTok{length}\NormalTok{(issues) ,}\StringTok{" categories it belongs: "}\NormalTok{,issues\_string,}\StringTok{". Use your judgement and only output a single issue category. The answer your need to categorize is: "}\NormalTok{, data}\SpecialCharTok{$}\NormalTok{open\_ended\_issue[i], }\StringTok{"."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The models were run on a desktop with 16GB of RAM, an Intel i5-4690K
CPU, and a GTX 1660 TI GPU. This relatively old hardware is still able
to run the models without any issues.

\subsection{Results}\label{results}

\subsubsection{Accuracy}\label{accuracy}

\begin{figure}[H]

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{graphs/accuracy.png}

}

\caption{Models accuracy in issue categorization}

\end{figure}%

\subsubsection{F-Score}\label{f-score}

\begin{table}[H]

\caption{F Scores for each model by issue category.}
\centering
\begin{tabular}[t]{lrrrrr}
\toprule
Issue Category & Llama3 & Phi3 & Mistral & GPT-4 & Dict\\
\midrule
Culture and Nationalism & NA & NA & 1.00 & NA & NA\\
Economy and Employment & 0.90 & 0.87 & NA & 0.94 & 0.21\\
Education & 0.67 & 0.67 & 1.00 & 0.67 & NA\\
Environment and Energy & 0.88 & 0.80 & 0.80 & 0.84 & 0.08\\
Governments and Governance & 0.41 & 0.47 & 0.56 & 0.65 & 0.03\\
\addlinespace
Health and Social Services & 0.94 & 0.83 & 0.91 & 0.96 & 0.34\\
Immigration & 1.00 & 1.00 & 1.00 & 1.00 & NA\\
Law and Crime & 1.00 & 1.00 & 1.00 & 1.00 & NA\\
Rights, Liberties, Minorities, and Discrimination & 0.86 & 0.86 & 0.71 & 0.57 & 0.29\\
Mean F-score & 0.83 & 0.81 & 0.87 & 0.83 & 0.19\\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Categorization Results}\label{categorization-results}

\section{Discussion}\label{discussion}

Open-ended question have a lot of potential. Beyond the fact that they
allow for more detailed and nuanced responses than closed-ended
questions, they can also help researchers to better understand the
attitudes and opinions of respondents through time and to identify
emerging issues and trends. Indeed, allowing respondents to answer
freely can help researchers to identify new issues and trends before
they start to appear in closed-ended questions.

Furthermore, being able to harness the power of large language models
locally, for free, can allow researchers working with sensitive data to
be compliant with strict ethical guidelines. Indeed, by running the
models locally, researchers can ensure that the data is not being shared
with third parties and that it is not being used for any other purposes
than the one intended. This can help to protect the privacy of
respondents and to ensure that the data is being used in a responsible
and ethical manner while still benefiting from the power of large
language models.

There are many more applications to be tested with open-ended questions
and large language models. For example, sentiment analysis, topic
modeling, and summarization could all be tested to see if they can be
used to analyze open-ended questions.

This solution doesn't solve all the problems related to open-ended
questions. It is still hard to find respondents willing to answer
open-ended questions, and the quality of the responses can vary greatly.

Close-ended question still have a role to play in survey research. They
are easier to analyze and quantify in bulk, and they can be used to
validate the results obtained by comparing them with other surveys
asking the same questions, they are practical in building scales and
indexes to measure latent variables.

Named entity recognition (NER) is a subtask of information extraction
that seeks to locate and classify named entities in text into predefined
categories such as names of persons, organizations, locations,
expressions of time, quantities, monetary values, percentages, etc. NER
is a crucial step in the analysis of open-ended questions as it allows
researchers to identify and extract relevant information from the text
\citep{yadav_bethard19}. By using large language models (LLMs), we can
streamline this process.

\section{Conclusion}\label{conclusion}

The method works but is still limited by how difficult is it to limit
the output of small models. Models were hard to prompt and outputs were
often ridden with irrelevant information. Cleaning the output
programmaticaly with conventional methods introduced many erroneous
results. The models could have scored better if the output was cleaned
manually. The issue was less present with the larger model, GPT-4-Turbo,
but it is not free to use and requires paid API access to run.

The next step of this projet is to test the capabilities of this method
on a whole survey where respondents are asked only open questions.

\newpage{}


\renewcommand\refname{References}
  \bibliography{bibliography.bib}


\end{document}
