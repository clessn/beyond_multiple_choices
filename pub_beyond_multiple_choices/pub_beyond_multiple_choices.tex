% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  authoryear,
  preprint,
  3p]{elsarticle}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{fvextra}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
\DefineVerbatimEnvironment{OutputCode}{Verbatim}{breaklines,commandchars=\\\{\}}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\journal{CPSA}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{elsarticle-harv}
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Beyond Multiple Choices},
  pdfauthor={Laurence-Olivier M. Foisy; Hubert Cadieux; Yannick Dufresne},
  pdfkeywords={Large Language Models, Open-Ended Survey
Questions, Survey Research, Open-Source Tools},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\setlength{\parindent}{6pt}
\begin{document}

\begin{frontmatter}
\title{Beyond Multiple Choices \\\large{Capturing Nuanced Public Opinion
with Large Language Models} }
\author[1]{Laurence-Olivier M. Foisy%
\corref{cor1}%
}
 \ead{mail@mfoisy} 
\author[1]{Hubert Cadieux%
%
}
 \ead{hubert.cadieux.1@ulaval.ca} 
\author[1]{Yannick Dufresne%
%
}
 \ead{yannick.dufresne@pol.ulaval.ca} 

\affiliation[1]{organization={Université Laval, Département de science
politique},addressline={2325 Rue de l'Université, Québec, QC G1V
0A6},city={Québec},postcode={G1V 0A6},postcodesep={}}

\cortext[cor1]{Corresponding author}



        
\begin{abstract}
Analyzing open-ended survey questions presents significant challenges
due to the diversity of responses and the manual effort required for
coding and categorization. This paper introduces a novel approach for
cleaning and analyzing open-ended questions using open-source large
language models (LLMs). Leveraging the R programming language and
Ollama's API, we demonstrate an efficient, cost-effective method for
processing qualitative data. Our approach enhances the ability to
extract meaningful insights from survey responses, providing a scalable
solution for researchers. By integrating open-source tools, we offer a
practical framework for transforming the analysis of open-ended
questions in survey research.
\end{abstract}





\begin{keyword}
    Large Language Models \sep Open-Ended Survey Questions \sep Survey
Research \sep 
    Open-Source Tools
\end{keyword}
\end{frontmatter}
    
\section{Introduction}\label{introduction}

Open-ended survey questions are notoriously difficult to analyze. In
1932, Rensis Likert published a seminal article in the Archives of
Psychology, introducing a new method to measure the intensity of
agreement or disagreement with a statement. The author underlined the
difficulty of measuring attitudes. He wrote that ``since it is possible
to group stimuli in almost any conceivable manner and to classify and
subclassify them indefinitely, it is strictly true that the number of
attitudes which any given person possesses is almost infinite''
\citep{likert32}. Today, most surveys are composed of a majority of
close-ended questions, and open-ended questions are rarely analyzed,
even if they are included in the survey \citep{Roberts_2014}. Indeed,
implementing open-ended questions in a survey comes with a host of
challenges. Respondents often skip them because they are time-consuming
and require more effort and reflection than closed-ended questions. It
can also be troublesome for mobile users to type lengthy and complex
responses \citep{dillman_etal14}. Open-ended questions are also
difficult to analyze because they require manual coding and
categorization of the responses. Indeed, many respondents can give the
same answer written in different ways. In a 2024 pilot survey about
lifestyle and health given to 2000 French and English Canadian
respondents\footnote{This survey was conducted by Leger's Leo panel.
  Administrated between April 24 and May 23, 2024. Quotas were put it
  place to maximize the survey's representativity}. People were asked,
``What is your favourite band or musician?'' The most popular answer
chosen by 75 respondents, The Beatles, was written in 10 different ways:
the beatles, The Beatles, The beatles, The Bwatles, beatles, Beatles,
beetles, Beetles, les beattels, and Les Beatles. Grammatical errors,
typos, and misspellings can make it difficult to analyze the data. While
it is not impossible to analyze open-ended questions, it is generally
time-consuming and expensive, especially when dealing with large
datasets
\citep{dillman_etal14, bradburn_etal04, roberts_etal14, schuman_presser96}.

However, open-ended questions can provide valuable insights into the
attitudes, opinions, and perceptions of respondents. They allow for more
detailed and nuanced responses than closed-ended questions. They avoid
the problem of forcing respondents to choose between a limited number of
options, which may not capture the full range of their opinions
\citep{dillman_etal14}. Open-ended questions can avoid cueing
respondents into thinking of issues in terms of particular causes or
treatments, which can be a problem with closed-ended questions
\citep{roberts_etal14, iyengar96}. Open-ended questions can help
researchers to better understand the attitudes and opinions of
respondents and to identify emerging issues and trends.

According to \citet{bickman_rog09}, open-ended questions align with
qualitative data, requiring in-depth analysis and interpretation, while
closed-ended questions are more suited to quantitative analysis due to
their ease of bulk analysis. This paper proposes utilizing current AI
technology to facilitate the analysis and quantification of open-ended
questions using the R programming language and Ollama's API. By doing
so, we can bridge the gap between qualitative data and quantitative
methods, enabling more comprehensive and insightful analysis from survey
respondents, ultimately enhancing our understanding of complex social
phenomena.

\section{Cleaning and Analyzing Open-Ended
Questions}\label{cleaning-and-analyzing-open-ended-questions}

\section{Research Question}\label{research-question}

Can open-source language models be trusted to accurately clean and
analyze open-ended survey questions? The goal is to provide a practical
solution for streamlining the analysis of open-ended questions in survey
research. By using open-source tools and integrating them into the
conventional social scientist's toolkit, we aim to enhance the ability
to extract meaningful insights from survey responses, providing an
easy-to-implement solution for researchers. The goal is not to replace
other methods of analyzing open-ended questions but to provide and
validate an alternative that could be used in conjunction with other
methods to improve the quality of the analysis.

\section{Methodology}\label{methodology}

\subsection{Ollama}\label{ollama}

Ollama is an open source platform that provides a user-friendly way of
downloading and running LLMs locally. It runs a server on the user's
machine that can be accessed through an API. Doing so allows the user to
interact with various LLMs without the need for extensive technical
expertise or reliance on cloud-based platforms. The Ollama API combined
with their library of pre-trained models is a powerful tool that can be
used to generate text, summarize documents, and perform a wide range of
other natural language processing tasks for free. The API is designed to
be easy to use and flexible, allowing users to customize their
interactions with LLMs to suit their needs. Using this tool, we can
potentially clean and analyze open-ended survey questions, even with
limited resources.

Ollama offers a wide range of models in their library. Smaller models
which can be used with a typical laptop and larger models requiring more
computational power and GPUs. Ollama recommends a minimum of 8GB of RAM
to run smaller 7 Billion parameters models and 16GB of RAM to run larger
13 Billion parameters models \citep{ollama24}.

\subsection{The CLELLM Package}\label{the-clellm-package}

For the purpose of this paper, we built a R package that allows
researchers to interact with various language models through Ollama's
API. A similar package has already been published on CRAN by
\citet{gruber_weber24}. Users can install it directly using
\texttt{install.packages("rollama")} However, the package presented in
this paper is a little bit more flexible, and intuitive to use for
non-ai-specialists, allowing to easily install new models and to
alternate between them between each prompts making it easier to pick the
best model for each respective task. Ollama has an extensive library of
models that can be used for a wide range of natural language processing
tasks.

To install the package, users can use the following function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{devtools}\SpecialCharTok{::}\FunctionTok{install\_github}\NormalTok{(}\StringTok{"clessn/clellm"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Linux users can use the following function to install ollama:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{clellm}\SpecialCharTok{::}\FunctionTok{install\_ollama}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Windows and MacOS users can download the ollama binary from the
\href{https://ollama.com/}{Ollama website} and install it manually. Once
Ollama is installed, the user can pull the models they want to use
directly in R with the following function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{clellm}\SpecialCharTok{::}\FunctionTok{ollama\_install\_model}\NormalTok{(}\StringTok{"model\_name"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The package provides a simple functions that allow users to interact
with LLMs through Ollama's API. It is designed to be easy to use and
flexible, allowing users to interact with a wide collection of
open-source models the same way they would typically interact with
OpenAI's GPT models.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{clellm}\SpecialCharTok{::}\FunctionTok{ollama\_prompt}\NormalTok{(}\StringTok{"prompt"}\NormalTok{, }\AttributeTok{model =} \StringTok{"model\_name"}\NormalTok{, }\AttributeTok{format =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{print\_result =} \ConstantTok{TRUE}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

The functions takes in four arguments, the prompt, the model to use, the
format of the output, and whether to print the result. The
\texttt{prompt} argument is the text that the model will use to generate
a response. \texttt{model} is the name of the model to use.
\texttt{format} is the format of the output, which can be either
``json'' or ``text''. The \texttt{print\_result} argument is a logical
value that determines whether the result should be printed to the
console. The function returns the response generated by the model.

\subsection{Cleaning Open-Ended
Questions}\label{cleaning-open-ended-questions}

Cleaning open-ended questions is a crucial step in the analysis of
survey data. It involves removing irrelevant information, correcting
errors, and standardizing the responses. This process can be
time-consuming and labor-intensive, especially when dealing with large
datasets. However, by using LLMs, we can automate much of this process,
making it faster and more efficient.

To validate if open-source LLMs with limited parameters can be used to
clean open-ended questions, we will run issue categorization on a subset
of 200 respondents drawn randomly from the 2021 Canadian Election Study.
The respondents were asked ``What is the most important issue to you
personally in this federal election?'' and could answer freely. The goal
of the analysis is to classify the responses into a set of
pre-determined issue categories. For this paper, the 12 issues from
ULaval's Center for Public Policy Analysis (CAPP) were used but any set
of categories could be used. A human coder classified the open-ended
responses into the 12 categories which will serve as a benchmark for the
model's performance. For the purpose of this paper, the score of the
human coder will be considered as the ground truth and set at 100\%. The
model's performance will be measured by comparing its classification to
the human coder's classification.

To validate if open-source large language models (LLMs) with limited
parameters can be used to clean open-ended questions, we will run issue
categorization on a subset of 200 respondents drawn randomly from the
2021 Canadian Election Study. The respondents were asked, ``What is the
most important issue to you personally in this federal election?'' and
could answer freely. The goal of the analysis is to classify the
responses into a set of predetermined issue categories. For this paper,
the 12 issues from ULaval's Center for Public Policy Analysis (CAPP)
were used, although any set of categories could be employed. Depending
on the needs of the research or the researcher, any set of categories
can be utilized, providing flexibility and adaptability in the analysis.

A human coder classified the open-ended responses into the 12
categories, which will serve as a benchmark for the model's performance.
For the purpose of this paper, the score of the human coder will be
considered the ground truth and set at 100\%. The model's performance
will be measured by comparing its classification to the human coder's
classification.

Three open-source models will be asked to classify the responses:
mistral from \citet{jiang_etal23}, llama3 from \citet{meta24}, and
phi3:mini from \citet{abdin_etal24}. For comparison, gpt-4-turbo from
\citet{openai23b}, the most capable model available on the market at the
moment of writing this paper, will also be given the same task. Although
the quality of its ouputs are notoriously superior to any other models,
it is not free to use and requires paid API access to run.

Each models will be asked to classify the responses into one of the 12
issue categories. They will all be given the exact same prompt.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prompt }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\StringTok{"In this survey question, respondents had to name their most important issue. Please read the answer and determine to which of the following "}\NormalTok{,}\FunctionTok{length}\NormalTok{(issues) ,}\StringTok{" categories it belongs: "}\NormalTok{,issues\_string,}\StringTok{". Use your judgement and only output a single issue category. The answer your need to categorize is: "}\NormalTok{, data}\SpecialCharTok{$}\NormalTok{open\_ended\_issue[i], }\StringTok{"."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The models were run on a desktop with 16GB of RAM, an Intel i5-4690K
CPU, and a GTX 1660 TI GPU. This relatively old hardware is still able
to run the models without any issues.

A dictionnary analysis will also be run on the responses for comparison
purposes. Dictionnary analysis are easy to implement and can be used to
quickly analyze open-ended questions. However, they are limited in scope
and can only be used to analyze responses that contain specific
keywords. They are not able to analyze responses that do not contain the
keywords, limiting their utility in analyzing short open-ended questions
without a lot of context. The dictionnary used for this analysis was
built by the CAPP and contains 1374 keywords for the 12 issues. They are
based on \citet{sevenans_etal14} Comparative Agenda Project and have
been modified to fit the Canadian context by a team of human coders from
the CAPP.

\subsection{Results}\label{results}

\begin{figure}

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{graphs/issue_distribution.png}

}

\caption{\label{fig-distribution}Distribution of Issues by the Human
Coder}

\end{figure}%

From the random sample of 200 respondents, the most important issue to
them personally in the 2021 Canadian federal election was the economy
and employment (n = 70), followed by health and social services (n =
53), and Governments and Governance (n = 34). The least important issues
that were not mentioned by any respondents were Technology,
International Affairs and Defense, and Public Lands and Agriculture.The
distribution of issues by the human coder is shown in
Figure~\ref{fig-distribution}.

\subsubsection{Accuracy}\label{accuracy}

To test the accuracy of each models, the percentage of correct
categorization was calculated. The results are shown in
Figure~\ref{fig-accuracy}. Without surprise, the larger model,
GPT-4-Turbo, scored the highest with 84\% accuracy. Llama3 scored at
79.5\%, followed by Phi3 at 75.5\%, and Mistral at 73.5\%. The
dictionnary analysis scored the lowest at 19\%. Those results are in
line with the expectations. The larger model, GPT-4-Turbo, is known to
be the most capable model available on the market at the moment of
writing this paper. The smaller models, Llama3, Phi3, and Mistral, are
open-source models with limited parameters. They are not as capable as
GPT-4-Turbo but are still able to perform the task. The dictionnary
analysis is limited in scope and does not perform well in short
open-ended questions.

\begin{figure}

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{graphs/accuracy.png}

}

\caption{\label{fig-accuracy}Models accuracy in issue categorization}

\end{figure}%

\subsubsection{F-Score}\label{f-score}

The F-score, also known as the F1-score, provides a more balanced
measure of a model's performance by considering both precision and
recall. Precision measures the proportion of correct positive
predictions made by the model, while recall measures the proportion of
actual positives that were correctly identified. The F-score is the
harmonic mean of precision and recall, which ensures that both metrics
are given equal weight.

The F-scores for each model across different issue categories are
summarized in Table~\ref{tbl-issues}:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.5747}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.0805}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.0575}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.0920}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.0690}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1264}}@{}}
\caption{F Scores for each model by issue
category.}\label{tbl-issues}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Issue Category
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Llama3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Phi3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Mistral
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
GPT-4
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Dictionary
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Issue Category
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Llama3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Phi3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Mistral
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
GPT-4
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Dictionary
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Culture and Nationalism & NA & NA & 1.00 & NA & NA \\
Economy and Employment & 0.90 & 0.87 & NA & 0.94 & 0.21 \\
Education & 0.67 & 0.67 & 1.00 & 0.67 & NA \\
Environment and Energy & 0.88 & 0.80 & 0.80 & 0.84 & 0.08 \\
Governments and Governance & 0.41 & 0.47 & 0.56 & 0.65 & 0.03 \\
Health and Social Services & 0.94 & 0.83 & 0.91 & 0.96 & 0.34 \\
Immigration & 1.00 & 1.00 & 1.00 & 1.00 & NA \\
Law and Crime & 1.00 & 1.00 & 1.00 & 1.00 & NA \\
Rights, Liberties, Minorities, and Discrimination & 0.86 & 0.86 & 0.71 &
0.57 & 0.29 \\
Mean F-score & 0.83 & 0.81 & 0.87 & 0.83 & 0.19 \\
\end{longtable}

The F-scores demonstrate that open-source models, particularly Mistral,
can effectively handle the categorization of open-ended survey
responses, approaching the performance of the more advanced but paid
GPT-4. The dictionary approach's consistently low scores underscore its
limitations and the necessity of using more advanced NLP techniques for
nuanced analysis.

These findings suggest that while GPT-4 remains the most accurate,
open-source models like Mistral, Llama3, and Phi3 are viable
alternatives for researchers, especially when cost or data privacy are
concerns. The results also highlight the variability in model
performance across different issue categories, emphasizing the
importance of choosing the right model based on the specific context and
requirements of the survey analysis.

\section{Discussion}\label{discussion}

Open-ended questions hold significant potential in survey research.
Beyond allowing for more detailed and nuanced responses than
closed-ended questions, they enable researchers to gain a deeper
understanding of the attitudes and opinions of respondents over time and
to identify emerging issues and trends. Allowing respondents to answer
freely can help researchers pinpoint new issues and trends before they
surface in closed-ended questions.

Furthermore, the ability to harness the power of large language models
(LLMs) locally and for free can enable researchers to work with
sensitive data while adhering to strict ethical guidelines. By running
the models locally, researchers can ensure that the data is not shared
with third parties and is used solely for its intended purpose. This
practice helps protect the privacy of respondents and ensures that the
data is handled responsibly and ethically, all while benefiting from the
capabilities of LLMs.

There are numerous additional applications to explore with open-ended
questions and LLMs, such as sentiment analysis, topic modeling, and
summarization. These applications could further enhance the analysis of
open-ended questions.

However, this solution does not address all the challenges associated
with open-ended questions. It remains difficult to find respondents
willing to answer open-ended questions, and the quality of the responses
can vary greatly. Close-ended questions still play a crucial role in
survey research. They are easier to analyze and quantify in bulk, and
they can be used to validate results by comparing them with other
surveys that ask the same questions. Additionally, close-ended questions
are practical for building scales and indexes to measure latent
variables.

Named entity recognition (NER) is another critical tool in the analysis
of open-ended questions. NER seeks to locate and classify named entities
in text into predefined categories such as names of persons,
organizations, locations, expressions of time, quantities, monetary
values, percentages, etc. NER is crucial for extracting relevant
information from text \citep{yadav_bethard19}. By using LLMs, we can
streamline the NER process, making it more efficient and effective.

\section{Conclusion}\label{conclusion}

This study demonstrates that utilizing large language models (LLMs) for
the analysis of open-ended survey questions is both viable and
promising. The results indicate that open-source models like Mistral,
Llama3, and Phi3 can effectively categorize responses, approaching the
performance levels of the more advanced GPT-4-Turbo model. This
capability is especially important for researchers working with limited
budgets or sensitive data, where cost and data privacy are significant
concerns.

However, the method is not without its limitations. Smaller models,
while capable, often produce outputs cluttered with irrelevant
information. This makes it challenging to prompt them effectively and
necessitates extensive cleaning, which, when done programmatically, can
introduce errors. Manual cleaning of the output could potentially
enhance the models' performance but at the cost of increased labor and
time. The larger GPT-4-Turbo model showed fewer issues in this regard,
but its usage comes with financial costs and dependency on paid API
access.

The potential of open-ended questions to capture detailed and nuanced
responses highlights their value in survey research. They offer insights
that closed-ended questions cannot, helping researchers understand
respondent attitudes and identify emerging trends. The ability to run
LLMs locally also ensures compliance with ethical guidelines,
safeguarding respondent privacy and data integrity.

Future research should focus on refining these methods and exploring
additional applications such as sentiment analysis, topic modeling, and
summarization. Moreover, it is essential to test the capabilities of
these models on larger surveys composed entirely of open-ended
questions. This would provide a comprehensive understanding of their
effectiveness and practical utility in real-world survey research
scenarios.

In conclusion, while there are challenges to overcome, the integration
of LLMs in the analysis of open-ended survey questions holds significant
promise. It represents a meaningful advancement in the toolkit available
to social scientists, offering a scalable and efficient solution to
transform qualitative data into actionable insights.

\newpage{}


\renewcommand\refname{References}
  \bibliography{bibliography.bib}


\end{document}
