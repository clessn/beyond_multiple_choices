---
title: Beyond Multiple Choices
subtitle: Capturing Nuanced Public Opinion with Large Language Models
author:
  - name: Laurence-Olivier M. Foisy
    email: mail@mfoisy
    affiliations: 
        - id: ULaval
          name: Université Laval
          department: Département de science politique
          address: 2325 Rue de l'Université, Québec, QC G1V 0A6
          city: Québec
          state: Québec
          postal-code: G1V 0A6
    attributes:
        corresponding: true
  - name: Hubert Cadieux
    email: hubert.cadieux.1@ulaval.ca
    affiliations:
        - id: ULaval
          name: Université Laval
          department: Département de science politique
          address: 2325 Rue de l'Université, Québec, QC G1V 0A6
          city: Québec
          state: Québec
          postal-code: G1V 0A6
  - name: Yannick Dufresne
    email: yannick.dufresne@pol.ulaval.ca
    affiliations:
        - id: ULaval
          name: Université Laval
          department: Département de science politique
          address: 2325 Rue de l'Université, Québec, QC G1V 0A6
          city: Québec
          state: Québec
          postal-code: G1V 0A6
abstract: |
  Analyzing open-ended survey questions presents significant challenges due to the diversity of responses and the manual effort required for coding and categorization. This paper introduces a novel approach for cleaning and analyzing open-ended questions using open-source large language models (LLMs). Leveraging the R programming language and Ollama's API, we demonstrate an efficient, cost-effective method for processing qualitative data. Our approach enhances the ability to extract meaningful insights from survey responses, providing a scalable solution for researchers. By integrating open-source tools, we offer a practical framework for transforming the analysis of open-ended questions in survey research.
keywords: 
  - keyword1
  - keyword2
date: last-modified
bibliography: bibliography.bib
format:
  elsevier-pdf:
    keep-tex: true
    journal:
      name: CPSA
      formatting: preprint
      model: 3p
      cite-style: authoryear
---

# Introduction

Open-ended survey questions are notoriously difficult to analyze. In 1932, Rensis Likert published a seminal article in the Archives of Psychology, introducing a new method to measure the intensity of agreement or disagreement with a statement. The author underlined the difficulty of measuring attitudes. He wrote that "since it is possible to group stimuli in almost any conceivable manner and to classify and subclassify them indefinitely, it is strictly true that the number of attitudes which any given person possesses is almost infinite" [@likert32]. Indeed, implementing open-ended questions in a survey comes with a host of challenges. Respondents often skip them because they are time-consuming and require more effort and reflection than closed-ended questions. It can also be troublesome for mobile users to type lengthy and complex responses [@dillman_etal14]. Open-ended questions are also difficult to analyze because they require manual coding and categorization of the responses. Indeed, many respondents can give the same answer written in different ways. In a 2024 pilot survey about lifestyle and health given to 2000 French and English Canadian respondents, people were asked, "What is your favourite band or musician?" The most popular answer chosen by 75 respondents, The Beatles, was written in 10 different ways: the beatles, The Beatles, The beatles, The Bwatles, beatles, Beatles, beetles, Beetles, les beattels, and Les Beatles. Grammatical errors, typos, and misspellings can make it difficult to analyze the data. While it is not impossible to analyze open-ended questions, it is generally time-consuming and expensive, especially when dealing with large datasets [@dillman_etal14; @bradburn_etal04].

However, open-ended questions can provide valuable insights into the attitudes, opinions, and perceptions of respondents. They allow for more detailed and nuanced responses than closed-ended questions. They avoid the problem of forcing respondents to choose between a limited number of options, which may not capture the full range of their opinions [@dillman_etal14]. Open-ended questions can help researchers to better understand the attitudes and opinions of respondents and to identify emerging issues and trends.

@bickman_rog09 relate open-ended questions to qualitative-data since they require deeper analysis and interpretation, and close-ended questions to quantitative data since they are easier to analyze and quantify in bulk. This paper use current AI technology to offer an easy method of analysis and quantification of open-ended questions with the use of the R programming language and Ollama's API, allowing the use of a wide array of open-source language models directly in the cleaning process, free of charge. This method can provide valuable insights into the data and help researchers to better understand the attitudes and opinions of respondents.

# Research Question

Can open-source language models be trusted to accurately clean and analyze open-ended survey questions? The goal is to provide a practical solution for streamlining the analysis of open-ended questions in survey research. By leveraging open-source tools and integrating them into a conventional social scientist's toolkit, we aim to enhance the ability to extract meaningful insights from survey responses, providing an easy-to-implement solution for researchers. The goal is not to replace other methods of analyzing open-ended questions but to provide and validate an alternative that could be used in conjunction with other methods to improve the quality of the analysis.

# Methodology

## Ollama

Ollama is an open source platform that provides a user-friendly way of downloading and running LLMs locally. It runs a server on the user's machine that can be accessed through an API. Doing so allows the user to interact with various LLMs without the need for extensive technical expertise or reliance on cloud-based platforms. The Ollama API combined with their library of pre-trained models is a powerful tool that can be used to generate text, summarize documents, and perform a wide range of other natural language processing tasks for free. The API is designed to be easy to use and flexible, allowing users to customize their interactions with LLMs to suit their needs. Using this tool, we can potentially clean and analyze open-ended survey questions, even with limited resources.

Ollama offers a wide range of models in their library. Smaller models which can be used with a typical laptop and larger models requiring more computational power and GPUs. Ollama recommends a minimum of 8GB of RAM to run smaller 7 Billion parameters models and 16GB of RAM to run larger 13 Billion parameters models [@ollama24].

## The CLELLM Package

For the purpose of this paper, we built a R package that allows researchers to interact with various language models through Ollama's API. A similar package has already been published on CRAN by @gruber_weber24. Users can install it directly using `install.packages("rollama")` However, the package presented in this paper is a little bit more flexible, and intuitive to use for non-ai-specialists, allowing to easily install new model and to alternate them between each prompts making it easier to pick the best model for each respective task.

To install the package, users can use the following function:

```r
devtools::install_github("clessn/clellm")
```

Linux users can use the following function to install ollama:

```r
clellm::install_ollama()
```
Windows and MacOS users can download the ollama binary from the [Ollama website](https://ollama.com/) and install it manually. Once Ollama is installed, the user can pull the models they want to use directly in R with the following function:

```r
clellm::ollama_install_model("model_name")
```

The package provides a simple functions that allow users to interact with LLMs through Ollama's API. It is designed to be easy to use and flexible, allowing users to interact with a wide collection of open-source models the same way they would typically interact with OpenAI's GPT models. 

```r
clellm::ollama_prompt("prompt", model = "model_name", format = NULL, print_result = TRUE) 
```
The functions takes in four arguments, the prompt, the model to use, the format of the output, and whether to print the result. The prompt is the text that the model will use to generate a response. The model is the name of the model to use. The format is the format of the output, which can be either "json" or "text". The print_result argument is a logical value that determines whether the result should be printed to the console. The function returns the response generated by the model.

## Cleaning Open-Ended Questions

### Issue Categorization

Cleaning open-ended questions is a crucial step in the analysis of survey data. It involves removing irrelevant information, correcting errors, and standardizing the responses. This process can be time-consuming and labor-intensive, especially when dealing with large datasets. However, by using LLMs, we can automate much of this process, making it faster and more efficient.

To validate if open-source LLMs with limited parameters can be used to clean open-ended questions, we will run issue categorization on a subset of 200 respondents drawn randomly from the 2021 Canadian Election Study. The respondents were asked "What is the most important issue to you personally in this federal election?" and could answer freely. The goal of the analysis is to classify the responses into a set of pre-determined issue categories. For this paper, the 12 issues from ULaval's Center for Public Policy Analysis (CAPP) were used but any set of categories could be used. A human coder classified the open-ended responses into the 12 categories which will serve as a benchmark for the model's performance. For the purpose of this paper, the score of the human coder will be considered as the ground truth and set at 100%. The model's performance will be measured by comparing its classification to the human coder's classification.

To validate if open-source large language models (LLMs) with limited parameters can be used to clean open-ended questions, we will run issue categorization on a subset of 200 respondents drawn randomly from the 2021 Canadian Election Study. The respondents were asked, "What is the most important issue to you personally in this federal election?" and could answer freely. The goal of the analysis is to classify the responses into a set of predetermined issue categories. For this paper, the 12 issues from ULaval's Center for Public Policy Analysis (CAPP) were used, although any set of categories could be employed. Depending on the needs of the research or the researcher, any set of categories can be utilized, providing flexibility and adaptability in the analysis.

A human coder classified the open-ended responses into the 12 categories, which will serve as a benchmark for the model's performance. For the purpose of this paper, the score of the human coder will be considered the ground truth and set at 100%. The model's performance will be measured by comparing its classification to the human coder's classification.

Three open-source models will be asked to classify the responses: mistral from @jiang_etal23, llama3 from @meta24, and phi3:mini from @abdin_etal24. For comparison, gpt-4-turbo from @openai23b, the most capable model available on the market at the moment of writing this paper, will also be given the same task. Although the quality of its ouputs are notoriously superior to any other models, it is not free to use and requires paid API access to run. 

Each models will be asked to classify the responses into one of the 12 issue categories. They will all be given the exact same prompt.
```r
prompt <- paste0("In this survey question, respondents had to name their most important issue. Please read the answer and determine to which of the following ",length(issues) ," categories it belongs: ",issues_string,". Use your judgement and only output a single issue category. The answer your need to categorize is: ", data$open_ended_issue[i], ".")
```
The models were run on a desktop with 16GB of RAM, an Intel i5-4690K CPU, and a GTX 1660 TI GPU. This relatively old hardware is still able to run the models without any issues.

#### Categorization Results

### Named Entity Recognition with Larger Prompts

Named entity recognition (NER) is a subtask of information extraction that seeks to locate and classify named entities in text into predefined categories such as names of persons, organizations, locations, expressions of time, quantities, monetary values, percentages, etc. NER is a crucial step in the analysis of open-ended questions as it allows researchers to identify and extract relevant information from the text [@yadav_bethard19]. By using large language models (LLMs), we can streamline this process.

We will try to feed LLaMA3, the open-source model that obtained the highest score in the issue categorization task, with a larger prompt to see if it can perform NER on the same subset of 200 respondents drawn randomly from the 2021 Canadian Election Study, and compare it with a human coder and with GPT-4-Turbo. The models were asked to extract the names of politicians, political parties, ministries, and organizations from the responses with the following prompt:

```r
prompt <- paste0(
    "You are an AI trained to identify named entities in text. ",
    "In the context of a Canadian election survey, respondents were asked, ",
    "\"What is the most important issue to you in this election?\" ",
    "Your task is to extract the named entities mentioned in each response, ",
    "categorizing them into appropriate types such as politicians, political parties, ministries, and organizations.\n\n",
  
    "Here are some example responses:\n\n",
  
    "1. \"I support the policies of Justin Trudeau.\"\n",
    "2. \"The Liberal Party's stance on healthcare is important to me.\"\n",
    "3. \"I believe the Ministry of Health needs to focus on mental health services.\"\n",
    "4. \"Organizations like Greenpeace are crucial for climate change advocacy.\"\n",
    "5. \"Andrew Scheer's approach to the economy resonates with me.\"\n\n",
  
    "For each response, identify and categorize the named entities as follows:\n\n",
  
    "Response 1:\n",
    "- Politicians: Justin Trudeau\n\n",
  
    "Response 2:\n",
    "- Political Parties: Liberal Party\n\n",
  
    "Response 3:\n",
    "- Ministries: Ministry of Health\n\n",
  
    "Response 4:\n",
    "- Organizations: Greenpeace\n\n",
  
    "Response 5:\n",
    "- Politicians: Andrew Scheer\n\n",
  
    "Now, extract the named entities for the following response:\n\n",
  
    "Response: \"", data$open_ended_issue[i], "\"\n\n",
  
    "Response 1:"
  )
```

#### Categorization Results

# Discussion

Open-ended question have a lot of potential. Beyond the fact that they allow for more detailed and nuanced responses than closed-ended questions, they can also help researchers to better understand the attitudes and opinions of respondents through time and to identify emerging issues and trends. Indeed, allowing respondents to answer freely can help researchers to identify new issues and trends before they start to appear in closed-ended questions. 

Furthermore, being able to harness the power of large language models locally, for free, can allow researchers working with sensitive data to be compliant with strict ethical guidelines. Indeed, by running the models locally, researchers can ensure that the data is not being shared with third parties and that it is not being used for any other purposes than the one intended. This can help to protect the privacy of respondents and to ensure that the data is being used in a responsible and ethical manner while still benefiting from the power of large language models.

There are many more applications to be tested with open-ended questions and large language models. For example, sentiment analysis, topic modeling, and summarization could all be tested to see if they can be used to analyze open-ended questions.

This solution doesn't solve all the problems related to open-ended questions. It is still hard to find respondents willing to answer open-ended questions, and the quality of the responses can vary greatly. 

Close-ended question still have a role to play in survey research. They are easier to analyze and quantify in bulk, and they can be used to validate the results obtained by comparing them with other surveys asking the same questions, they are practical in building scales and indexes to measure latent variables.

# Conclusion

The method works but is still limited by how difficult is it to limit the output of small models. Models were hard to prompt and outputs were often ridden with irrelevant information. Cleaning the output programmaticaly with conventional methods introduced many erroneous results. The models could have scored better if the output was cleaned manually. The issue was less present with the larger model, GPT-4-Turbo, but it is not free to use and requires paid API access to run.

The next step of this projet is to test the capabilities of this method on a whole survey where respondents are asked only open questions.

{{< pagebreak >}}
# References {-}
