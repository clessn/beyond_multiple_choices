---
title: Beyond Multiple Choices
subtitle: Capturing Nuanced Public Opinion with Large Language Models
author:
  - name: Laurence-Olivier M. Foisy
    email: mail@mfoisy
    affiliations: 
        - id: ULaval
          name: Université Laval
          department: Département de science politique
          address: 2325 Rue de l'Université, Québec, QC G1V 0A6
          city: Québec
          state: Québec
          postal-code: G1V 0A6
    attributes:
        corresponding: true
  - name: Hubert Cadieux
    email: hubert.cadieux.1@ulaval.ca
    affiliations:
        - id: ULaval
          name: Université Laval
          department: Département de science politique
          address: 2325 Rue de l'Université, Québec, QC G1V 0A6
          city: Québec
          state: Québec
          postal-code: G1V 0A6
  - name: Étienne Proulx
    email: etienne.proulx.2@ulaval.ca
    affiliations:
        - id: ULaval
          name: Université Laval
          department: Département de science politique
          address: 2325 Rue de l'Université, Québec, QC G1V 0A6
          city: Québec
          state: Québec
          postal-code: G1V 0A6
  - name: Yannick Dufresne
    email: yannick.dufresne@pol.ulaval.ca
    affiliations:
        - id: ULaval
          name: Université Laval
          department: Département de science politique
          address: 2325 Rue de l'Université, Québec, QC G1V 0A6
          city: Québec
          state: Québec
          postal-code: G1V 0A6
abstract: |
  Analyzing open-ended survey questions presents significant challenges due to the diversity of responses and the manual effort required for coding and categorization. This paper introduces a new approach for cleaning and analyzing open-ended questions using open-source large language models (LLMs). Using the R programming language and Ollama's API, we demonstrate an efficient, cost-effective method for processing qualitative data. Our approach enhances the ability to extract meaningful insights from survey responses, providing a scalable solution for researchers. By integrating open-source tools, we offer a practical framework for transforming the analysis of open-ended questions in survey research.
keywords: 
  - Large Language Models
  - Open-Ended Survey Questions
  - Survey Research
  - Open-Source Tools
date: last-modified
bibliography: bibliography.bib
format:
  elsevier-pdf:
    keep-tex: true
    journal:
      name: CPSA
      formatting: preprint
      model: 3p
      cite-style: authoryear
---

# Introduction

Open-ended survey questions are notoriously difficult to analyze. In 1932, Rensis Likert published a seminal article in the Archives of Psychology, introducing a new method to measure the intensity of agreement or disagreement with a statement. The author underlined the difficulty of measuring attitudes since "it is possible to group stimuli in almost any conceivable manner and to classify and subclassify them indefinitely, it is strictly true that the number of attitudes which any given person possesses is almost infinite" [@likert32]. Today, most surveys are composed of a majority of close-ended questions and open-ended questions are rarely analyzed, even if they are included in the survey [@roberts_etal14]. Indeed, implementing open-ended questions in a survey comes with a host of challenges. Respondents often skip them because they are time-consuming and require more effort and reflection than closed-ended questions. It can also be troublesome for mobile users to type lengthy and complex responses [@dillman_etal14]. Open-ended questions are also difficult to analyze because they often require manual coding and categorization of the responses. Indeed, many respondents can give the same answer written in different ways. In a 2024 pilot survey about lifestyle and health given to 2000 French and English Canadian respondents^[This survey was conducted by Leger's Leo panel and financed by the Center for Public Policy Analysis (CAPP). Deployed between April 24 and May 23, 2024. Quotas were put in place to maximize the survey's representativity]. People were asked, "What is your favourite band or musician?" The most popular answer chosen by 75 respondents, The Beatles, was written in 10 different ways: the beatles, The Beatles, The beatles, The Bwatles, beatles, Beatles, beetles, Beetles, les beattels, and Les Beatles. Grammatical errors, typos, and misspellings can make it difficult to analyze the data. While it is not impossible to analyze open-ended questions, it is generally time-consuming and expensive, especially when dealing with large datasets [@dillman_etal14; @bradburn_etal04; @roberts_etal14;@schuman_presser96].

However, open-ended questions can provide valuable insights into the attitudes, opinions, and perceptions of respondents thus making them valuable for measuring public opinion. They allow for more detailed and nuanced responses than closed-ended questions. They avoid the problem of forcing respondents to choose between a limited number of options, which may not capture the full range of their opinions [@dillman_etal14]. Open-ended questions can avoid cueing respondents into thinking of issues in terms of particular causes or treatments, which can be a problem with closed-ended questions [@roberts_etal14; @iyengar96]. Open-ended questions can help researchers to better understand the attitudes and opinions of respondents and to identify emerging issues and trends. Since respondents can answer freely, researchers can pinpoint new issues and trends before they surface in closed-ended questions, allowing for a more organic and nuanced understanding of public opinion.

According to @bickman_rog09, open-ended questions align with qualitative data, requiring in-depth analysis and interpretation, while closed-ended questions are more suited to quantitative analysis due to their ease of bulk analysis. This paper proposes utilizing current AI technology to facilitate the analysis and quantification of open-ended questions using the R programming language and Ollama's API. By doing so, we can bridge the gap between qualitative data and quantitative methods, enabling more comprehensive and insightful analysis from survey respondents, and ultimately enhancing our understanding of complex social phenomena. 

# Cleaning and Analyzing Open-Ended Questions

There are many ways to analyze open-ended questions. The most popular method is to manually code and categorize the responses. This method is time-consuming, labor-intensive, and prone to human error, but it is also the most accurate and trustworthy [@roberts_etal14]. Programmatical methods can also be used to streamline analysis with various degrees of precision. @grimmer_etal13 lists various text as data methods to utilize depending on their use case as shown in @fig-methods.

```{mermaid}
%%| fig-width: 100%
%%| label: fig-methods
%%| fig-cap: "Grimmer (2013) overview of text as data methods."
flowchart TD
    A[Acquire Documents]
    B[Preprocess]
    C[Research Objective]
    D[Classification]
    E[Ideological Scaling]
    E1[Supervised - Wordscore]
    E2[Unsupervised - Wordfish]
    F[Known Categories]
    G[Unknown Categories]
    F1[Dictionary Methods]
    F2[Supervised Methods]
    F21[Individual Classification]
    F22[Measuring Proportions - ReadMe]
    F211[Individual Methods]
    F212[Ensembles]
    G1[Fully Automated Clustering]
    G2[Computer Assisted Clustering]
    G11[Single Membership Models]
    G12[Mixed Membership Models]
    G111[Document Level - LDA]
    G121[Date Level - Dynamic Multitopic Model]
    G122[Author Level - Expressed Agenda Model]

    A --> B
    B --> C
    C --> D
    C --> E
    E --> E1
    E --> E2
    D --> F
    D --> G
    F --> F1
    F --> F2
    F2 --> F21
    F2 --> F22
    F21 --> F211
    F21 --> F212
    G --> G1
    G --> G2
    G1 --> G11
    G1 --> G12
    G11 --> G111
    G12 --> G121
    G12 --> G122
```

These automated methods require various degrees of effort to learn, understand, and implement. The goal of this paper is not to replace any of these methods or to pretend that large language models analysis is superior, but to provide an easy-to-use versatile alternative, that can be implemented in different contexts, and that can be used in conjunction with other methods to improve the quality of the analysis.

# Research Question

Can open-source language models be trusted to reliably clean and analyze open-ended survey questions? The objective is to provide a practical solution for streamlining the analysis of open-ended questions in survey research. By using open-source tools and integrating them into the conventional social scientist's toolkit, we aim to enhance the ability to extract meaningful insights from survey responses, providing an easy-to-implement solution for researchers. This is a preliminary study to test the feasibility of using open-source language models for the analysis of open-ended survey questions. The results will provide insights into the potential of this approach and its limitations, guiding future research in this area.

# Methodology

## Ollama

Ollama is an open-source platform that provides a user-friendly way of downloading and running LLMs locally. It runs a server on the user's machine that can be accessed through an API. Doing so allows the user to interact with various LLMs without the need for extensive technical expertise or reliance on cloud-based platforms. The Ollama API combined with their library of pre-trained models is a powerful tool that can be used to generate text, summarize documents, and perform a wide range of other natural language processing tasks for free. The API is designed to be easy to use and flexible, allowing users to customize their interactions with LLMs to suit their needs. Using this tool, we can potentially clean and analyze open-ended survey questions, even with limited resources.

Ollama offers a wide range of models in its library. Smaller models which can run on a typical researcher's laptop, and larger models requiring more computational power. Ollama recommends a minimum of 8GB of RAM to run smaller 7 Billion parameter models and 16GB of RAM to run larger 13 Billion parameter models [@ollama24].

## The CLELLM Package

For the purpose of this paper, we built an R package that allows researchers to interact with various language models through Ollama's API^[The package is accessible at https://github.com/clessn/clellm. Feel free to modify it and send pull requests.]. A similar package has already been published on CRAN by @gruber_weber24. Users can install it directly using `install.packages("rollama")`. However, we think that the package presented in this paper is a little bit more flexible and intuitive to use for non-ai-specialists. It allows users to easily install new models and to alternate between them between each prompt, making it easier to pick the best model for each respective task. Ollama has an extensive library of models that can be used for a wide range of natural language processing tasks.

To install the package, users can use the following function:

```r
devtools::install_github("clessn/clellm")
```

Linux users can use the following function to install Ollama:

```r
clellm::install_ollama()
```
Windows and MacOS users can download the Ollama binary from the [Ollama website](https://ollama.com/) and install it manually. Once Ollama is installed, the user can pull the models they want to use directly in R with the following function:

```r
clellm::ollama_install_model("model_name")
```

The package provides a simple functions allowing users to interact with LLMs through Ollama's API. It is designed to be easy to use and flexible, allowing users to interact with a wide collection of open-source models the same way they would typically interact with OpenAI's GPT models. 

```r
clellm::ollama_prompt("prompt", model = "model_name")
```
The function takes in four arguments, the prompt, the model to use, the format of the output, and whether to print the result in the console. The `prompt` argument is the text that the model will use to generate a response. `model` is the name of the model to use. `format` is the format of the output, which can be either `"json"` or `"text"`. The `print_result` argument is a logical value that determines whether the result should be printed to the console. The function returns the response generated by the model.

## Cleaning Open-Ended Questions

Cleaning open-ended questions is a crucial step in the analysis of survey data. It involves removing irrelevant information, correcting errors, and standardizing the responses. To validate if open-source LLMs with limited parameters can be used to clean open-ended questions, we will run issue categorization on a subset of 200 respondents drawn randomly from the 2021 Canadian Election Study. The respondents were asked "What is the most important issue to you personally in this federal election?" and could answer freely. The goal of the analysis is to classify the responses into a set of pre-determined issue categories. For this paper, the 12 issues from Université Laval's Center for Public Policy Analysis (CAPP) were used, but any set of categories could be used. The 12 issues were: Law and Crime, Culture and Nationalism, Public Lands and Agriculture, Governments and Governance, Immigration", "Rights, Liberties, Minorities, and Discrimination, Health and Social Services, Economy and Employment, Education, Environment and Energy, International Affairs and Defense, and Technology. A human coder classified the open-ended responses into the 12 categories which will serve as a benchmark for the model's performance. For the purpose of this paper, the score of the human coder will be considered as the ground truth and set at 100%. The model's performance will be measured by comparing its classification to the human coder's classification.

Three open-source models will be asked to classify the responses: mistral from @jiang_etal23, llama3 from @meta24, and phi3:mini from @abdin_etal24. For comparison, gpt-4-turbo from @openai23b, the most capable model available on the market at the moment of writing this paper, will also be given the same task. Although the quality of its outputs is notoriously superior to any other models, it is not free to use and requires paid API access to run.

Each model will be asked to classify the responses into one of the 12 issue categories. They will all be given the exact same prompt.
```r
prompt <- paste0("In this survey question, respondents had to name their most important issue. Please read the answer and determine to which of the following ",length(issues) ," categories it belongs: ",issues_string,". Use your judgement and only output a single issue category. The answer your need to categorize is: ", data$open_ended_issue[i], ".")
```
The models were run on a desktop with 16GB of RAM, an Intel i5-4690K CPU, and a GTX 1660 TI GPU and a laptop with 16GB of RAM and and an intel i7-8650u CPU without GPU. This relatively old hardware is still able to run the models without any issues.

A dictionary analysis will also be conducted on the responses for comparison purposes. Dictionary analyses are easy to implement and can be used to quickly analyze open-ended questions. However, they are limited in scope and can only be used to analyze responses that contain specific keywords. They are not able to analyze responses that do not contain the keywords, limiting their utility in analyzing short open-ended questions without a lot of context. This method is expected to perform poorly in this context. The dictionary used for this analysis was built by the CAPP and contains 1374 keywords for the 12 issues. They are based on @sevenans_etal14 Comparative Agenda Project and have been modified to fit the Canadian context by a team of human coders from the CAPP.

## Results

![Distribution of Issues by the Human Coder](graphs/issue_distribution.png){#fig-distribution width=80% fig-align="center" fig-pos="H"}

From the random sample of 200 respondents, the most important issue to them in the 2021 Canadian federal election was the economy and employment (n = 70), followed by health and social services (n = 53), and Governments and Governance (n = 34). The least important issues that were not mentioned by any respondents were Technology, International Affairs and Defense, and Public Lands and Agriculture. The distribution of issues by the human coder is shown in @fig-distribution.

### Accuracy

To test the accuracy of each model, the percentage of correct categorization was calculated. The results are shown in @fig-accuracy. The larger model, GPT-4-Turbo, scored the highest with 84% accuracy. Llama3 scored at 79.5%, followed by Phi3 at 75.5%, and Mistral at 73.5%. The dictionary analysis scored the lowest at 19%. Those results are in line with the expectations: The larger model, GPT-4-Turbo, is the most capable model available on the market at the moment of writing this paper. The smaller models, Llama3, Phi3, and Mistral, are open-source models with limited parameters. They are not as capable as GPT-4-Turbo but are still able to perform the task. The dictionary analysis is limited in scope and does not perform well in short open-ended questions. 

![Models accuracy in issue categorization](graphs/accuracy.png){#fig-accuracy width=80% fig-align="center" fig-pos="H"}

### F-Score

Accuracy is often not the best metric to evaluate the performance of a model. One of its disadvantages is that it does not provide information on the performance of a classifier in each class separately [@obi23]. It also does not account for when there is an imbalance of classes in the dataset, like in the case of our dataset where economy and employment is by far the most mentioned issue. The F-score, also known as the F1-score, provides a more balanced measure of a model's performance by considering both precision and recall. Precision measures the proportion of correct positive predictions made by the model, while recall measures the proportion of actual positives that were correctly identified. The F-score is the harmonic mean of precision and recall, which ensures that both metrics are given equal weight [@christen_etal24].

The F-scores for each model across different issue categories are summarized in @tbl-issues:

::: {#tbl-issues}

|Issue Category                                    |Llama3 |Phi3 |Mistral |GPT-4 |Dict |
|:-------------------------------------------------|:------|:----|:-------|:-----|:----|
|Culture and Nationalism                           |NA     |NA   |1       |NA    |NA   |
|Economy and Employment                            |0.9    |0.87 |NA      |0.94  |0.21 |
|Education                                         |0.67   |0.67 |1       |0.67  |NA   |
|Environment and Energy                            |0.88   |0.8  |0.8     |0.84  |0.08 |
|Governments and Governance                        |0.41   |0.47 |0.56    |0.65  |0.03 |
|Health and Social Services                        |0.94   |0.83 |0.91    |0.96  |0.34 |
|Immigration                                       |1      |1    |1       |1     |NA   |
|Law and Crime                                     |1      |1    |1       |1     |NA   |
|Rights, Liberties, Minorities, and Discrimination |0.86   |0.86 |0.71    |0.57  |0.29 |
|Weighted Mean for Issue Frequency                 |0.81   |0.77 |0.5     |0.86  |0.19 |

F Scores for each model by issue category.

:::

The F-scores demonstrate that open-source models, particularly Llama3, can effectively handle the categorization of open-ended survey responses, approaching the performance of the more advanced but paid GPT-4. The dictionary approach's consistently low scores underscore its limitations in this context and the necessity of using more advanced NLP techniques for nuanced analysis.

These findings suggest that while GPT-4 remains the most accurate, open-source models like Llama3, Phi3, and Mistral are viable alternatives for researchers, especially when cost or data privacy are concerned. The results also highlight the variability in model performance across different issue categories, emphasizing the importance of choosing the right model based on the specific context and requirements of the survey analysis.

# Discussion

Open-ended questions hold significant potential in survey research. Beyond allowing for more detailed and nuanced responses than closed-ended questions, they enable researchers to gain a deeper understanding of the attitudes and opinions of respondents over time and to identify emerging issues and trends. Allowing respondents to answer freely can help researchers pinpoint new issues and trends before they surface in closed-ended questions.

Furthermore, the ability to harness the power of large language models (LLMs) locally and for free can enable researchers to work with sensitive data while adhering to strict ethical guidelines. By running the models locally, researchers can ensure that the data is not shared with third parties and is used solely for its intended purpose. This practice helps protect the privacy of respondents and ensures that the data is handled responsibly and ethically, all while benefiting from the capabilities of LLMs.

There are numerous additional applications to explore with open-ended questions and LLMs, such as sentiment analysis, topic modeling, and summarization. These applications could further enhance the analysis of open-ended questions. For example, Named entity recognition (NER), a critical tool in the analysis of open-ended questions, could benefit from LLM analysis. NER seeks to locate and classify named entities in text into predefined categories such as names of persons, organizations, locations, expressions of time, quantities, monetary values, percentages, etc. NER is crucial for extracting relevant information from text [@yadav_bethard19]. By using LLMs, we could streamline the NER process, making it more efficient, effective, and accessible to researchers.

However, this solution does not address all the challenges associated with open-ended questions. It remains difficult to find respondents willing to answer open-ended questions, and the quality of the responses can vary greatly. Close-ended questions still play a crucial role in survey research. They are easier to analyze and quantify in bulk, and they can be used to validate results by comparing them with other surveys that ask the same questions. They are also easier for respondents to answer, which can help increase response rates. Additionally, close-ended questions are practical for building scales and indexes to measure latent variables.

# Conclusion

This preliminary study demonstrates that utilizing large language models (LLMs) for the analysis of open-ended survey questions is both viable and promising. The results indicate that open-source models like Mistral, Llama3, and Phi3 can effectively categorize responses, approaching the performance levels of the more advanced GPT-4-Turbo model. pThis capability is especially important for researchers working with limited budgets or sensitive data, where cost and data privacy are significant concerns.

However, the method is not without its limitations. Smaller models, while capable, often produce outputs cluttered with irrelevant information. This makes it challenging to prompt them effectively and necessitates extensive cleaning, which, when done programmatically, can introduce errors. Manual cleaning of the output could potentially enhance the models' performance but at the cost of increased labor and time. The larger GPT-4-Turbo model showed fewer issues in this regard, but its usage comes with financial costs and dependency on paid API access.

The potential of open-ended questions to capture detailed and nuanced responses highlights their value in survey research. They offer insights that closed-ended questions cannot, helping researchers understand respondent attitudes and identify emerging trends. The ability to run LLMs locally also ensures compliance with ethical guidelines, safeguarding respondent privacy and data integrity.

Future research should focus on refining these methods and exploring additional applications such as sentiment analysis, topic modeling, and summarization. Moreover, it would be interesting to test the capabilities of these models on larger surveys composed entirely of open-ended questions and observe if it is possible to build scales and measure latent variables in this way. This would provide a comprehensive understanding of their effectiveness and practical utility in real-world survey research scenarios. This study represents a preliminary step in this direction, offering a practical framework for integrating LLMs into the analysis of open-ended survey questions. 

In conclusion, while there are challenges to overcome, the integration of LLMs in the analysis of open-ended survey questions holds significant promise. It represents a meaningful advancement in the toolkit available to social scientists, offering a scalable and efficient solution to transform qualitative data into actionable insights. This technology, still in its infancy, has a lot of room for growth and improvement. LLM technology might not yet be ready to replace human coders but their potential applications in survey research are vast and exciting.

The code and data used for this project are available at [https://github.com/clessn/beyond_multiple_choices](https://github.com/clessn/beyond_multiple_choices).

{{< pagebreak >}}
# References {-}
