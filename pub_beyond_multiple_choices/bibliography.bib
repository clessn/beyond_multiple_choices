@misc{abdin_etal24,
  title = {Phi-3 {{Technical Report}}: {{A Highly Capable Language Model Locally}} on {{Your Phone}}},
  shorttitle = {Phi-3 {{Technical Report}}},
  author = {Abdin, Marah and Jacobs, Sam Ade and Awan, Ammar Ahmad and Aneja, Jyoti and Awadallah, Ahmed and Awadalla, Hany and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl, Harkirat and Benhaim, Alon and Bilenko, Misha and Bjorck, Johan and Bubeck, S{\'e}bastien and Cai, Qin and Cai, Martin and Mendes, Caio C{\'e}sar Teodoro and Chen, Weizhu and Chaudhary, Vishrav and Chen, Dong and Chen, Dongdong and Chen, Yen-Chun and Chen, Yi-Ling and Chopra, Parul and Dai, Xiyang and Del Giorno, Allie and {de Rosa}, Gustavo and Dixon, Matthew and Eldan, Ronen and Fragoso, Victor and Iter, Dan and Gao, Mei and Gao, Min and Gao, Jianfeng and Garg, Amit and Goswami, Abhishek and Gunasekar, Suriya and Haider, Emman and Hao, Junheng and Hewett, Russell J. and Huynh, Jamie and Javaheripi, Mojan and Jin, Xin and Kauffmann, Piero and Karampatziakis, Nikos and Kim, Dongwoo and Khademi, Mahoud and Kurilenko, Lev and Lee, James R. and Lee, Yin Tat and Li, Yuanzhi and Li, Yunsheng and Liang, Chen and Liden, Lars and Liu, Ce and Liu, Mengchen and Liu, Weishung and Lin, Eric and Lin, Zeqi and Luo, Chong and Madan, Piyush and Mazzola, Matt and Mitra, Arindam and Modi, Hardik and Nguyen, Anh and Norick, Brandon and Patra, Barun and {Perez-Becker}, Daniel and Portet, Thomas and Pryzant, Reid and Qin, Heyang and Radmilac, Marko and Rosset, Corby and Roy, Sambudha and Ruwase, Olatunji and Saarikivi, Olli and Saied, Amin and Salim, Adil and Santacroce, Michael and Shah, Shital and Shang, Ning and Sharma, Hiteshi and Shukla, Swadheen and Song, Xia and Tanaka, Masahiro and Tupini, Andrea and Wang, Xin and Wang, Lijuan and Wang, Chunyu and Wang, Yu and Ward, Rachel and Wang, Guanhua and Witte, Philipp and Wu, Haiping and Wyatt, Michael and Xiao, Bin and Xu, Can and Xu, Jiahang and Xu, Weijian and Yadav, Sonali and Yang, Fan and Yang, Jianwei and Yang, Ziyi and Yang, Yifan and Yu, Donghan and Yuan, Lu and Zhang, Chengruidong and Zhang, Cyril and Zhang, Jianwen and Zhang, Li Lyna and Zhang, Yi and Zhang, Yue and Zhang, Yunan and Zhou, Xiren},
  year = {2024},
  month = may,
  number = {arXiv:2404.14219},
  eprint = {2404.14219},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.14219},
  urldate = {2024-06-05},
  abstract = {We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69\% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in our dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered publicly available web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called phi-3-small and phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75\% and 78\% on MMLU, and 8.7 and 8.9 on MT-bench). Moreover, we also introduce phi-3-vision, a 4.2 billion parameter model based on phi-3-mini with strong reasoning capabilities for image and text prompts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/ral/Zotero/storage/K5TL8ZKQ/Abdin et al_2024_Phi-3 Technical Report.pdf;/home/ral/Zotero/storage/ATC7BYU6/2404.html}
}

@book{bickman_rog09,
  title = {The {{SAGE}} Handbook of Applied Social Research Methods},
  editor = {Bickman, Leonard and Rog, Debra J.},
  year = {2009},
  edition = {2nd ed},
  publisher = {SAGE},
  address = {Los Angeles},
  isbn = {978-1-4129-5031-2},
  langid = {english},
  lccn = {H62 .H24534 2009},
  keywords = {Methodology,Research Methodology,Social sciences},
  annotation = {OCLC: ocn212893577},
  file = {/home/ral/Zotero/storage/R8WRFFDY/Bickman and Rog - 2009 - The SAGE handbook of applied social research metho.pdf}
}

@book{bradburn_etal04,
  title = {Asking Questions: The Definitive Guide to Questionnaire Design - for Market Research, Political Polls, and Social and Health Questionnaires},
  shorttitle = {Asking Questions},
  author = {Bradburn, Norman M. and Sudman, Seymour and Wansink, Brian},
  year = {2004},
  edition = {Rev. ed},
  publisher = {Jossey-Bass},
  address = {San Francisco, Calif},
  isbn = {978-0-7879-7088-8},
  langid = {english},
  file = {/home/ral/Zotero/storage/3IZ7JYQ8/N M Bradburn Et Al Asking Questions The De - Unknown.pdf}
}

@article{christen_etal24,
  title = {A {{Review}} of the {{F-Measure}}: {{Its History}}, {{Properties}}, {{Criticism}}, and {{Alternatives}}},
  shorttitle = {A {{Review}} of the {{F-Measure}}},
  author = {Christen, Peter and Hand, David J. and Kirielle, Nishadi},
  year = {2024},
  month = mar,
  journal = {ACM Computing Surveys},
  volume = {56},
  number = {3},
  pages = {1--24},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3606367},
  urldate = {2024-06-06},
  abstract = {Methods to classify objects into two or more classes are at the core of various disciplines. When a set of objects with their true classes is available, a supervised classifier can be trained and employed to decide if, for example, a new patient has cancer or not. The choice of performance measure is critical in deciding which supervised method to use in any particular classification problem. Different measures can lead to very different choices, so the measure should match the objectives. Many performance measures have been developed, and one of them is the F-measure, the harmonic mean of precision and recall. Originally proposed in information retrieval, the F-measure has gained increasing interest in the context of classification. However, the rationale underlying this measure appears weak, and unlike other measures, it does not have a representational meaning. The use of the harmonic mean also has little theoretical justification. The F-measure also stresses one class, which seems inappropriate for general classification problems. We provide a history of the F-measure and its use in computational disciplines, describe its properties, and discuss criticism about the F-Measure. We conclude with alternatives to the F-measure, and recommendations of how to use it effectively.},
  langid = {english},
  file = {/home/ral/Zotero/storage/A4LUGJF9/Christen et al. - 2024 - A Review of the F-Measure Its History, Properties.pdf}
}

@article{chung_etal22,
  title = {Text-Mining Open-Ended Survey Responses Using Structural Topic Modeling: A Practical Demonstration to Understand Parents' Coping Methods during the {{COVID-19}} Pandemic in Singapore},
  author = {Chung, Gerard and Chung, Gerard and Rodriguez, Maria Y. and Rodriguez, Maria Y. and Lanier, Paul and Lanier, Paul and Gibbs, Daniel and Gibbs, Daniel},
  year = {2022},
  journal = {Journal of Technology in Human Services},
  doi = {10.1080/15228835.2022.2036301},
  abstract = {Open-ended survey questions crucially contribute to researchers' understandings of respondents' experiences. However, analyzing open-ended responses using human coders is labor-intensive. Structural topic modeling (STM) is a text mining method that discover topics from textual data. We demonstrate the use of STM to analyze open-ended survey responses to understand how parents coped during the COVID-19 lock-down in Singapore. We administered online surveys to 199 parents in Singapore during the COVID-19 lock-down. To show a STM analysis, we demonstrated a workflow that includes steps in data preprocessing, model estimation, model selection, and model interpretation. An 18-topic model best fit the data based on model diagnostics and researchers' expertise. Prevalent coping methods described by respondents include ``Spousal Support,'' ``Routines/Schedules,'' and ``Managing Expectations.'' Topic prevalence for some topics varied with respondents' levels of parenting stress and whether parents were fathers or mothers. STM offers an efficient, valid, and replicable way to analyze textual data such as open-ended survey responses and case notes that can complement researchers' knowledge and skills. STM can be used as part of a multistage research process or to support other analyses such as clarifying quantitative findings and identifying preliminary themes from qualitative data.Supplemental data for this article is available online at https://doi.org/10.1080/15228835.2022.2036301 .},
  mag_id = {4213091819},
  pmcid = {null},
  pmid = {null},
  file = {/home/ral/Zotero/storage/98BUVFV5/Chung et al_2022_Text-mining open-ended survey responses using structural topic modeling.pdf}
}

@book{dillman_etal14,
  title = {Internet, {{Phone}}, {{Mail}}, and {{Mixed}}-{{Mode Surveys}}: {{The Tailored Design Method}}},
  shorttitle = {Internet, {{Phone}}, {{Mail}}, and {{Mixed}}-{{Mode Surveys}}},
  author = {Dillman, Don A and Smyth, Jolene D and Christian, Leah Melani},
  year = {2014},
  month = aug,
  edition = {1},
  publisher = {Wiley},
  doi = {10.1002/9781394260645},
  urldate = {2024-06-01},
  isbn = {978-1-394-26064-5},
  langid = {english},
  file = {/home/ral/Zotero/storage/GFY5Y8M9/Dillman et al. - 2014 - Internet, Phone, Mail, and Mixed‚ÄêMode Surveys The.pdf}
}

@book{fowler14,
  title = {Survey Research Methods},
  author = {Fowler, Floyd J.},
  year = {2014},
  series = {Applied Social Research Methods Series},
  edition = {Fifth edition},
  publisher = {SAGE},
  address = {Los Angeles},
  isbn = {978-1-4522-5900-0 978-1-4833-1240-8},
  langid = {english},
  lccn = {HN29 .F68 2014},
  keywords = {Social surveys},
  file = {/home/ral/Zotero/storage/FDJ8Z26T/Fowler - 2014 - Survey research methods.pdf}
}

@article{grimmer_etal13,
  title = {Text as Data: {{The}} Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts},
  author = {Grimmer, Justin and Grimmer, Justin and Stewart, Brandon and Stewart, Brandon M.},
  year = {2013},
  journal = {Political Analysis},
  doi = {10.1093/pan/mps028},
  abstract = {Politics and political conflict often occur in the written and spoken word. Scholars have long recognized this, but the massive costs of analyzing even moderately sized collections of texts have hindered their use in political science research. Here lies the promise of automated text analysis: it substantially reduces the costs of analyzing large collections of text. We provide a guide to this exciting new area of research and show how, in many instances, the methods have already obtained part of their promise. But there are pitfalls to using automated methods---they are no substitute for careful thought and close reading and require extensive and problem-specific validation. We survey a wide range of new methods, provide guidance on how to validate the output of the models, and clarify misconceptions and errors in the literature. To conclude, we argue that for automated text methods to become a standard tool for political scientists, methodologists must contribute new methods and new methods of validation. Language is the medium for politics and political conflict. Candidates debate and state policy positions during a campaign. Once elected, representatives write and debate legislation. After laws are passed, bureaucrats solicit comments before they issue regulations. Nations regularly negotiate and then sign peace treaties, with language that signals the motivations and relative power of the countries involved. News reports document the day-to-day affairs of international relations that provide a detailed picture of conflict and cooperation. Individual candidates and political parties articulate their views through party platforms and manifestos. Terrorist groups even reveal their preferences and goals through recruiting materials, magazines, and public statements. These examples, and many others throughout political science, show that to understand what politics is about we need to know what political actors are saying and writing. Recognizing that language is central to the study of politics is not new. To the contrary, scholars of politics have long recognized that much of politics is expressed in words. But scholars have struggled when using texts to make inferences about politics. The primary problem is volume: there are simply too many political texts. Rarely are scholars able to manually read all the texts in even moderately sized corpora. And hiring coders to manually read all documents is still very expensive. The result is that},
  mag_id = {2095655043},
  pmcid = {null},
  pmid = {null},
  file = {/home/ral/Zotero/storage/2DMC3PER/Grimmer et al_2013_Text as data.pdf}
}

@misc{gruber_weber24,
  title = {Rollama: {{An R}} Package for Using Generative Large Language Models through {{Ollama}}},
  shorttitle = {Rollama},
  author = {Gruber, Johannes B. and Weber, Maximilian},
  year = {2024},
  month = apr,
  number = {arXiv:2404.07654},
  eprint = {2404.07654},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-01},
  abstract = {rollama is an R package that wraps the Ollama API, which allows you to run different Generative Large Language Models (GLLM) locally. The package and learning material focus on making it easy to use Ollama for annotating textual or imagine data with open-source models as well as use these models for document embedding. But users can use or extend rollama to do essentially anything else that is possible through OpenAI's API, yet more private, reproducible and for free.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/ral/Zotero/storage/HM4JSXJH/Gruber and Weber - 2024 - rollama An R package for using generative large l.pdf}
}

@article{iyengar96,
  title = {Framing {{Responsibility}} for {{Political Issues}}},
  author = {Iyengar, Shanto},
  year = {1996},
  month = jul,
  journal = {The ANNALS of the American Academy of Political and Social Science},
  volume = {546},
  number = {1},
  pages = {59--70},
  publisher = {SAGE Publications Inc},
  issn = {0002-7162},
  doi = {10.1177/0002716296546001006},
  urldate = {2024-06-06},
  abstract = {This article examines the influence of television news on viewers' attributions of responsibility for political issues. Television's systematic reliance on episodic as opposed to thematic depictions of political life elicits individualistic attributions of responsibility for national problems such as poverty and terrorism. These attributions emphasize the actions of private rather than governmental actors. By obscuring the connections between political problems and the actions or inactions of political leaders, television news trivializes political discourse and weakens the accountability of elected officials.},
  langid = {english}
}

@misc{jiang_etal23,
  title = {Mistral {{7B}}},
  author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and de las Casas, Diego and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and Lavaud, L{\'e}lio Renard and Lachaux, Marie-Anne and Stock, Pierre and Scao, Teven Le and Lavril, Thibaut and Wang, Thomas and Lacroix, Timoth{\'e}e and Sayed, William El},
  year = {2023},
  month = oct,
  number = {arXiv:2310.06825},
  eprint = {2310.06825},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.06825},
  urldate = {2024-06-04},
  abstract = {We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/ral/Zotero/storage/C6G277ZT/Jiang et al_2023_Mistral 7B.pdf;/home/ral/Zotero/storage/KT9ICMPL/2310.html}
}

@article{joshi_etal15,
  title = {Likert {{Scale}}: {{Explored}} and {{Explained}}},
  shorttitle = {Likert {{Scale}}},
  author = {Joshi, Ankur and Kale, Saket and Chandel, Satish and Pal, D.},
  year = {2015},
  month = jan,
  journal = {British Journal of Applied Science \& Technology},
  volume = {7},
  number = {4},
  pages = {396--403},
  issn = {22310843},
  doi = {10.9734/BJAST/2015/14975},
  urldate = {2024-06-01},
  abstract = {Likert scale is applied as one of the most fundamental and frequently used psychometric tools in educational and social sciences research. Simultaneously, it is also subjected to a lot of debates and controversies in regards with the analysis and inclusion of points on the scale. With this context, through reviewing the available literature and then clubbing the received information with coherent scientific thinking, this paper attempts to gradually build a construct around Likert scale. This analytical review begins with the necessity of psychometric tools like Likert scale andits variants and focuses on some convoluted issues like validity, reliability and analysis of the scale.},
  langid = {english},
  file = {/home/ral/Zotero/storage/T2CIRCMG/Joshi et al. - 2015 - Likert Scale Explored and Explained.pdf}
}

@book{likert32,
  title = {A Technique for the Measurement of Attitudes},
  author = {Likert, R.},
  year = {1932},
  series = {A Technique for the Measurement of Attitudes},
  number = {nos. 136-165},
  publisher = {Archives of Psychology},
  lccn = {33012634},
  file = {/home/ral/Zotero/storage/HQKV2UCI/Likert_1932.pdf}
}

@misc{meta24,
  title = {Introducing {{Meta Llama}} 3: {{The}} Most Capable Openly Available {{LLM}} to Date},
  shorttitle = {Introducing {{Meta Llama}} 3},
  author = {{Meta}},
  year = {2024},
  journal = {Meta AI},
  urldate = {2024-06-05},
  abstract = {Today, we're introducing Meta Llama 3, the next generation of our state-of-the-art open source large language model. In the coming months, we expect to share new capabilities, additional model sizes, and more.},
  howpublished = {https://ai.meta.com/blog/meta-llama-3/},
  langid = {english}
}

@article{obi23,
  title = {A Comparative Study of Several Classification Metrics and Their Performances on Data},
  author = {Obi, Jude Chukwura},
  year = {2023},
  month = feb,
  journal = {World Journal of Advanced Engineering Technology and Sciences},
  volume = {8},
  number = {1},
  pages = {308--314},
  issn = {25828266},
  doi = {10.30574/wjaets.2023.8.1.0054},
  urldate = {2024-06-06},
  abstract = {Six classification metrics namely, Accuracy, Precision, Recall (Sensitivity), Specificity, F1-Score and Area Under the Curve have been studied in this work. A classification model based on the Support Vector Machine, was used to obtain a confusion matrix, which provided the needed information for calculating the different classification metrics. Twenty different datasets were used to assess the performances of the classification metrics. Accuracy and Area Under the Curve are the two metrics that consistently gave a classification result given each dataset used in the study. Although accuracy appears to be marginally better that AUC, it was discovered that in some cases where sensitivity is zero, accuracy yielded a high correct classification result. This goes further to implying that prior to choosing accuracy as a preferred metric for classification, investigation should be carried out to find out what sensitivity and specificity are. Where there are high values for sensitivity and specificity, the study shows that a choice of accuracy as a preferred classification metric leads to a high percentage of correct classification result.},
  file = {/home/ral/Zotero/storage/AR36RTK3/Jude Chukwura Obi_2023_A comparative study of several classification metrics and their performances on.pdf}
}

@misc{ollama24,
  title = {Ollama/Ollama},
  author = {{Ollama}},
  year = {2024},
  month = jun,
  urldate = {2024-06-02},
  abstract = {Get up and running with Llama 3, Mistral, Gemma, and other large language models.},
  copyright = {MIT},
  howpublished = {Ollama},
  keywords = {gemma,go,golang,llama,llama2,llama3,llava,llm,llms,mistral,ollama,phi3}
}

@techreport{openai23b,
  title = {{{GPT-4 Technical Report}}},
  author = {{OpenAI}},
  year = {2023},
  pages = {100},
  institution = {OpenAI},
  urldate = {2024-06-05},
  abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer- based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
  file = {/home/ral/Zotero/storage/9MGLMHI6/gpt-4.pdf}
}

@article{pietsch_lessmann18a,
  title = {Topic Modeling for Analyzing Open-Ended Survey Responses},
  author = {Pietsch, Andra-Selina and Lessmann, Stefan},
  year = {2018},
  month = jul,
  journal = {Journal of Business Analytics},
  volume = {1},
  number = {2},
  pages = {93--116},
  issn = {2573-234X, 2573-2358},
  doi = {10.1080/2573234X.2019.1590131},
  urldate = {2024-06-06},
  abstract = {Open-ended responses are widely used in market research studies. Processing of such responses requires labour-intensive human coding. This paper focuses on unsupervised topic models and tests their ability to automate the analysis of open-ended responses. Since state-of-the-art topic models struggle with the shortness of open-ended responses, the paper considers three novel short text topic models: Latent Feature Latent Dirichlet Allocation, Biterm Topic Model and Word Network Topic Model. The models are fitted and evaluated on a set of real-world open-ended responses provided by a market research company. Multiple components such as topic coherence and document classification are quantitatively and qualitatively evaluated to appraise whether topic models can replace human coding. The results suggest that topic models are a viable alternative for openended response coding. However, their usefulness is limited when a correct one-to-one mapping of responses and topics or the exact topic distribution is needed.},
  langid = {english},
  file = {/home/ral/Zotero/storage/GD3MLZDS/Pietsch and Lessmann - 2018 - Topic modeling for analyzing open-ended survey res.pdf}
}

@misc{Questions24,
  title = {Questions and {{Answers}} in {{Attitude Surveys}}},
  year = {2024},
  month = mar,
  journal = {SAGE Publications Inc},
  urldate = {2024-06-06},
  abstract = {Experiments on Question Form, Wording, and Context},
  howpublished = {https://us.sagepub.com/en-us/nam/questions-and-answers-in-attitude-surveys/book4010},
  langid = {english},
  file = {/home/ral/Zotero/storage/TLUT2EF3/book4010.html}
}

@article{roberts_etal14,
  title = {Structural {{Topic Models}} for {{Open-Ended Survey Responses}}},
  author = {Roberts, Margaret E. and Stewart, Brandon M. and Tingley, Dustin and Lucas, Christopher and {Leder-Luis}, Jetson and Gadarian, Shana Kushner and Albertson, Bethany and Rand, David G.},
  year = {2014},
  journal = {American Journal of Political Science},
  volume = {58},
  number = {4},
  pages = {1064--1082},
  issn = {1540-5907},
  doi = {10.1111/ajps.12103},
  urldate = {2023-09-03},
  abstract = {Collection and especially analysis of open-ended survey responses are relatively rare in the discipline and when conducted are almost exclusively done through human coding. We present an alternative, semiautomated approach, the structural topic model (STM) (Roberts, Stewart, and Airoldi 2013; Roberts et al. 2013), that draws on recent developments in machine learning based analysis of textual data. A crucial contribution of the method is that it incorporates information about the document, such as the author's gender, political affiliation, and treatment assignment (if an experimental study). This article focuses on how the STM is helpful for survey researchers and experimentalists. The STM makes analyzing open-ended responses easier, more revealing, and capable of being used to estimate treatment effects. We illustrate these innovations with analysis of text from surveys and experiments.},
  copyright = {{\copyright}2014, Midwest Political Science Association},
  langid = {english},
  keywords = {notion},
  file = {/home/ral/Zotero/storage/JZKYWF6B/Roberts et al_2014_Structural Topic Models for Open-Ended Survey Responses.pdf;/home/ral/Zotero/storage/ER9SHKWA/ajps.html}
}

@book{schuman_presser96,
  title = {Questions and Answers in Attitude Surveys: Experiments on Question Form, Wording, and Context},
  shorttitle = {Questions and Answers in Attitude Surveys},
  author = {Schuman, Howard and Presser, Stanley},
  year = {1996},
  publisher = {Sage Publications},
  address = {Thousand Oaks, CA},
  isbn = {978-0-7619-0359-8},
  lccn = {HN29 .S338 1996},
  keywords = {Research,Social sciences,Social surveys}
}

@inproceedings{sevenans_etal14,
  title = {The {{Automated Coding}} of {{Policy Agendas}}: {{A Dictionary Based Approach}}},
  shorttitle = {The {{Automated Coding}} of {{Policy Agendas}}},
  booktitle = {The {{Automated Coding}} of {{Policy Agendas}}: {{A Dictionary Based Approach}}},
  author = {Sevenans, Julie and Albaugh, Quinn and Shahaf, Tal and Soroka, Stuart and Walgrave, Stefaan},
  year = {2014},
  month = jun,
  pages = {1--27},
  address = {Konstanz, Germany},
  abstract = {For the coding of political and media texts, the Policy Agendas community has mostly taken a human coding approach, or they have turned to automated machine learning methods. Last year, we proposed an alternative dictionary-based approach for the automated content analysis of texts (see Albaugh et al. 2013). We designed a first version of an English and a Dutch CAP-dictionary, and we validated the results of the codings against human coded documents. Although the results were not perfect yet -- with respect to certain topic codes our dictionaries could certainly be improved -- we showed that dictionaries may produce reliable, valid and comparable measures of policy and media agendas. This year, we take the next step by doing three things. First, we further develop the Dutch and English dictionaries and try to replicate human coding results by making the dictionary assign single topic codes to single items. Second, we compare the dictionaries not only with human coded texts; we also validate them in a substantive manner. We expect individual Members of Parliament (MPs) to act most upon issues that they prioritize. Concretely, we test this by comparing MPs' dictionary-coded attention for issues with their committee membership. Third, we show that valuable insights can be gained from using the dictionaries in practice.},
  file = {/home/ral/Zotero/storage/BFCUW5ZS/Sevenans et al_2014_The Automated Coding of Policy Agendas.pdf}
}

@misc{tian_etal24,
  title = {{{SpreadsheetLLM}}: {{Encoding Spreadsheets}} for {{Large Language Models}}},
  shorttitle = {{{SpreadsheetLLM}}},
  author = {Tian, Yuzhang and Zhao, Jianbo and Dong, Haoyu and Xiong, Junyu and Xia, Shiyu and Zhou, Mengyu and Lin, Yun and Cambronero, Jos{\'e} and He, Yeye and Han, Shi and Zhang, Dongmei},
  year = {2024},
  month = jul,
  number = {arXiv:2407.09025},
  eprint = {2407.09025},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-19},
  abstract = {Spreadsheets, with their extensive two-dimensional grids, various layouts, and diverse formatting options, present notable challenges for large language models (LLMs). In response, we introduce SpreadsheetLLM, pioneering an efficient encoding method designed to unleash and optimize LLMs' powerful understanding and reasoning capability on spreadsheets. Initially, we propose a vanilla serialization approach that incorporates cell addresses, values, and formats. However, this approach was limited by LLMs' token constraints, making it impractical for most applications. To tackle this challenge, we develop SheetCompressor, an innovative encoding framework that compresses spreadsheets effectively for LLMs. It comprises three modules: structural-anchor-based compression, inverse index translation, and data-format-aware aggregation. It significantly improves performance in spreadsheet table detection task, outperforming the vanilla approach by 25.6\% in GPT4's in-context learning setting. Moreover, fine-tuned LLM with SheetCompressor has an average compression ratio of 25 times, but achieves a state-of-the-art 78.9\% F1 score, surpassing the best existing models by 12.3\%. Finally, we propose Chain of Spreadsheet for downstream tasks of spreadsheet understanding and validate in a new and demanding spreadsheet QA task. We methodically leverage the inherent layout and structure of spreadsheets, demonstrating that SpreadsheetLLM is highly effective across a variety of spreadsheet tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/ral/Zotero/storage/YESJQ5DD/Tian et al_2024_SpreadsheetLLM.pdf;/home/ral/Zotero/storage/TRGNXWDS/2407.html}
}

@misc{yadav_bethard19,
  title = {A {{Survey}} on {{Recent Advances}} in {{Named Entity Recognition}} from {{Deep Learning}} Models},
  author = {Yadav, Vikas and Bethard, Steven},
  year = {2019},
  month = oct,
  number = {arXiv:1910.11470},
  eprint = {1910.11470},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-05},
  abstract = {Named Entity Recognition (NER) is a key component in NLP systems for question answering, information retrieval, relation extraction, etc. NER systems have been studied and developed widely for decades, but accurate systems using deep neural networks (NN) have only been introduced in the last few years. We present a comprehensive survey of deep neural network architectures for NER, and contrast them with previous approaches to NER based on feature engineering and other supervised or semi-supervised learning algorithms. Our results highlight the improvements achieved by neural networks, and show how incorporating some of the lessons learned from past work on feature-based NER systems can yield further improvements.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/ral/Zotero/storage/2VKWYK38/Yadav_Bethard_2019_A Survey on Recent Advances in Named Entity Recognition from Deep Learning.pdf}
}
